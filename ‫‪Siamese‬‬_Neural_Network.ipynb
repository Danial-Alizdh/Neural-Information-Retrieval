{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 19:57:16.950167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 19:57:19.039021: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-22 19:57:19.039372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-22 19:57:19.039406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from math import sqrt, pow\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_AD = './Data/test_data.csv'\n",
    "train_data_AD = './Data/train_data.csv'\n",
    "valid_data_AD = './Data/valid_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "\n",
    "file = open(test_data_AD)\n",
    "\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "header = []\n",
    "header = next(csvreader)\n",
    "header\n",
    "\n",
    "test_data = []\n",
    "for row in csvreader:\n",
    "        test_data.append(row)\n",
    "test_data\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data\n",
    "\n",
    "file = open(train_data_AD)\n",
    "\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "header = []\n",
    "header = next(csvreader)\n",
    "header\n",
    "\n",
    "train_data = []\n",
    "for row in csvreader:\n",
    "        train_data.append(row)\n",
    "train_data\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Data\n",
    "\n",
    "file = open(valid_data_AD)\n",
    "\n",
    "csvreader = csv.reader(file)\n",
    "\n",
    "header = []\n",
    "header = next(csvreader)\n",
    "header\n",
    "\n",
    "valid_data = []\n",
    "for row in csvreader:\n",
    "        valid_data.append(row)\n",
    "valid_data\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37250\n",
      "16663\n"
     ]
    }
   ],
   "source": [
    "qid2 = []\n",
    "for i in range(len(train_data)) :\n",
    "    qid2.append(train_data[i][2])\n",
    "\n",
    "unique_res = list(np.unique(qid2))\n",
    "\n",
    "train_sentence = []\n",
    "for id in unique_res :\n",
    "    index = qid2.index(id)\n",
    "    train_sentence.append(train_data[index][4].lower())\n",
    "\n",
    "print(len(qid2))\n",
    "print(len(train_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16641\n",
      "16641\n"
     ]
    }
   ],
   "source": [
    "indexes = [train_sentence.index(x) for x in set(train_sentence)]\n",
    "train_sentence = list(set(train_sentence))\n",
    "\n",
    "print(len(indexes))\n",
    "print(len(train_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16641"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_qid2 = []\n",
    "for i in indexes :\n",
    "    unique_qid2.append(unique_res[i])\n",
    "\n",
    "len(unique_qid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "qid1 = []\n",
    "for i in range(len(test_data)) :\n",
    "    qid1.append(test_data[i][1])\n",
    "\n",
    "unique_qid1 = list(np.unique(qid1))\n",
    "\n",
    "test_sentence = []\n",
    "for id in unique_qid1 :\n",
    "    index = qid1.index(id)\n",
    "    test_sentence.append(test_data[index][3].lower())\n",
    "\n",
    "print(len(qid1))\n",
    "print(len(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Valid questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "qid3 = []\n",
    "for i in range(len(valid_data)) :\n",
    "    qid3.append(valid_data[i][1])\n",
    "\n",
    "unique_qid3 = list(np.unique(qid3))\n",
    "\n",
    "valid_sentence = []\n",
    "for id in unique_qid3 :\n",
    "    index = qid3.index(id)\n",
    "    valid_sentence.append(valid_data[index][4].lower())\n",
    "\n",
    "print(len(qid3))\n",
    "print(len(valid_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(precision) :\n",
    "    if len(precision) == 0 :\n",
    "        return 0\n",
    "    \n",
    "    s = 0\n",
    "    for pre in precision :\n",
    "        if len(pre) > 0 :\n",
    "            x = 0\n",
    "            for i in pre :\n",
    "                x += i\n",
    "            s += ( x / len(pre) )\n",
    "        \n",
    "    return s / len(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision5(precision) :\n",
    "    if len(precision) < 5 :\n",
    "        return 0\n",
    "    return precision[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision10(precision) :\n",
    "    if len(precision) < 10 :\n",
    "        return 0\n",
    "    return precision[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRR(reciprocal_rank, size) :\n",
    "    if len(reciprocal_rank) == 0 :\n",
    "        return 0\n",
    "    \n",
    "    s = 0\n",
    "    for rr in reciprocal_rank :\n",
    "        s += rr\n",
    "\n",
    "    return s / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessment(vector) :\n",
    "    precision = []\n",
    "    reciprocal_rank = []\n",
    "    for i in range(len(vector)) :\n",
    "        sort_index = np.argsort(vector[i])[::-1]\n",
    "        sort_index = sort_index[0:10]\n",
    "\n",
    "        index_pos_list = [ j for j in range(len(qid1)) if qid1[j] == unique_qid1[i] ]\n",
    "        test_s = [test_data[j][-2].lower() for j in index_pos_list]\n",
    "    \n",
    "        pr = []\n",
    "        sum = 0\n",
    "        h = 0\n",
    "        flag = True\n",
    "        for index in sort_index :\n",
    "            for z in range(len(train_data)) :\n",
    "              if qid2[z] == unique_qid2[index] :\n",
    "                train_s = train_data[z][-2].lower()\n",
    "                if train_s in test_s :\n",
    "                    sum += 1\n",
    "                    pr.append(sum/(h + 1))\n",
    "                    if flag :\n",
    "                        flag = False\n",
    "                        reciprocal_rank.append(1/(h + 1))\n",
    "                h += 1\n",
    "        precision.append(pr)\n",
    "    \n",
    "    \n",
    "    for i in range(len(precision)) :\n",
    "        print('Precision@5 for query ' + str(i) + ' = ' + str(Precision5(precision[i])) )\n",
    "        print('Precision@10 for query ' + str(i) + ' = ' + str(Precision10(precision[i])) )\n",
    "    \n",
    "    print('MAP = ' + str(MAP(precision)) )\n",
    "    print('MRR = ' + str(MRR(reciprocal_rank, len(precision))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(document) :\n",
    "    tokens = [list(tokenize(doc, lower=True)) for doc in document]\n",
    "    tokens = [preprocess_string(\" \".join(doc), [remove_stopwords, stem_text]) for doc in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words2(sentences) :\n",
    "    for i in range(len(sentences)) :\n",
    "        sentences[i] = remove_stopwords(sentences[i].lower())\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07914685  0.13173008  0.03685642  0.03331754  0.05163226 -0.17804459\n",
      "  0.08465194  0.21255319 -0.07343437 -0.06076741 -0.07861451 -0.1378414\n",
      " -0.01576091  0.04419099  0.05559474 -0.09130788  0.02907021 -0.14109035\n",
      "  0.01210359 -0.1850436   0.02004358  0.03893198  0.06305601 -0.02319679\n",
      " -0.04326034  0.00591453 -0.10462487 -0.05074805 -0.1504182   0.01525654\n",
      "  0.06918819 -0.01018891  0.02317539 -0.09506267 -0.06039242  0.11180805\n",
      "  0.0095405  -0.10466725 -0.0509086  -0.18092896 -0.02450538 -0.08663201\n",
      " -0.07712972 -0.00080951  0.10765804 -0.04551841 -0.08286542 -0.01110378\n",
      "  0.08274718  0.06305473  0.01414887 -0.14069507 -0.05170505 -0.04127173\n",
      " -0.06244693  0.06728996  0.08358322  0.05085017 -0.09391297  0.07103299\n",
      "  0.05802524  0.0247754  -0.04756982  0.00287238 -0.10527526  0.05204038\n",
      " -0.02731605  0.06806077 -0.1577914   0.13544388 -0.08401571  0.07078189\n",
      "  0.12895425 -0.08100188  0.14813639  0.02573707 -0.06374004 -0.01801891\n",
      " -0.0839019   0.01346428 -0.04224009 -0.02892512 -0.10465116  0.17586303\n",
      "  0.02441107  0.01469236  0.01391892  0.1294935   0.12834539 -0.03804391\n",
      "  0.12188689 -0.00095167  0.04309871  0.02683242  0.18688278  0.10467257\n",
      "  0.04268817 -0.10322991  0.01356168 -0.02951943]\n",
      "6634\n"
     ]
    }
   ],
   "source": [
    "sentences = remove_stop_words(train_sentence)\n",
    "sentences.extend(remove_stop_words(valid_sentence))\n",
    "\n",
    "# train word2vec model\n",
    "model_w2v = Word2Vec(sentences, min_count=1, vector_size=100, window=5, epochs=10)\n",
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# access vector for one word\n",
    "print(model_w2v.wv['zombi'])\n",
    "\n",
    "# get other similar words\n",
    "# sims = model.wv.most_similar('zombi', topn=10)\n",
    "\n",
    "\n",
    "#list the vocabulary words\n",
    "words_w2v = list(model_w2v.wv.index_to_key)\n",
    "\n",
    "print(len(words_w2v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05475635  0.0542057  -0.05570682  0.20506376  0.07276792 -0.07162417\n",
      "  0.06998584  0.05062641  0.11029191 -0.17169708 -0.01079341 -0.05211193\n",
      " -0.14049539  0.2224241  -0.0438219   0.00732591 -0.08386838 -0.08949397\n",
      " -0.16162866 -0.10113554 -0.1742477   0.01439965 -0.08123017 -0.05248114\n",
      "  0.03663433  0.00742886 -0.04388787 -0.00440948  0.12816195 -0.0104226\n",
      " -0.04942304  0.01094825  0.13993567 -0.0802836   0.01883134  0.05084011\n",
      " -0.02443593 -0.02598076 -0.11517685  0.05543267  0.15443198 -0.18191348\n",
      "  0.10086079 -0.04423262 -0.06545889  0.01509274  0.00989179 -0.1522388\n",
      "  0.00166808  0.15546855 -0.02202454 -0.05413138 -0.05568917  0.06896815\n",
      "  0.0367025  -0.03525211  0.00708335 -0.07333439  0.0201804  -0.10147867\n",
      "  0.20119257  0.07586314 -0.19908682  0.03630971  0.10305119  0.1624028\n",
      "  0.01664571  0.02902112  0.10150111  0.13323516  0.08815565  0.02426819\n",
      "  0.04080858 -0.1305007   0.15211287 -0.0443965   0.10403296 -0.05128287\n",
      " -0.09929251  0.06320065 -0.08365934 -0.11501861  0.03647066 -0.05673005\n",
      " -0.1016598  -0.00704862 -0.04599021  0.00539392 -0.03518711  0.115272\n",
      " -0.10497378 -0.08539233 -0.08115867  0.12281035 -0.15902416  0.20574453\n",
      " -0.03822719 -0.13818388 -0.00396816  0.18681855]\n",
      "6634\n"
     ]
    }
   ],
   "source": [
    "sentences = remove_stop_words(train_sentence)\n",
    "sentences.extend(remove_stop_words(valid_sentence))\n",
    "\n",
    "model_ft = FastText(vector_size=100, window=5, min_count=1, sentences=sentences, epochs=10)\n",
    "# model.save(\"word2vec.model\")\n",
    "# model = FastText.load(\"word2vec.model\")\n",
    "\n",
    "# access vector for one word\n",
    "print(model_ft.wv['zombi'])\n",
    "\n",
    "# get other similar words\n",
    "# sims = model.wv.most_similar('zombi', topn=10)\n",
    "\n",
    "\n",
    "#list the vocabulary words\n",
    "words_ft = list(model_ft.wv.index_to_key)\n",
    "\n",
    "print(len(words_ft))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text[i:i + seq_length]\n",
    " seq_out = raw_text[i + seq_length]\n",
    " dataX.append([char_to_int[char] for char in seq_in])\n",
    " dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "# You can now define your LSTM model. \n",
    "# Here, you define a single hidden LSTM layer with 256 memory units. \n",
    "# The network uses dropout with a probability of 20. \n",
    "# The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "# You can now fit your model to the data. Here, \n",
    "# you use a modest number of 20 epochs and a large batch size of 128 patterns.\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network loss decreased almost every epoch, so the network could likely benefit from training for many more epochs.\n",
    "\n",
    "# In the next section, you will look at using this model to generate new text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text[i:i + seq_length]\n",
    " seq_out = raw_text[i + seq_length]\n",
    " dataX.append([char_to_int[char] for char in seq_in])\n",
    " dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    " x = np.reshape(pattern, (1, len(pattern), 1))\n",
    " x = x / float(n_vocab)\n",
    " prediction = model.predict(x, verbose=0)\n",
    " index = np.argmax(prediction)\n",
    " result = int_to_char[index]\n",
    " seq_in = [int_to_char[value] for value in pattern]\n",
    " sys.stdout.write(result)\n",
    " pattern.append(index)\n",
    " pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will keep the number of memory units the same at 256 but add a second layer.\n",
    "\n",
    "# ...\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
